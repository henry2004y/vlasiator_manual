<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/vlasiator_manual/libs/katex/katex.min.css"> <link rel=stylesheet  href="/vlasiator_manual/libs/highlight/github.min.css"> <link rel=stylesheet  href="/vlasiator_manual/css/jtd.css"> <link rel=icon  href="/vlasiator_manual/assets/favicon.ico"> <title>Trouble Shooting</title> <div class=page-wrap > <div class=side-bar > <div class=header > <a href="/vlasiator_manual/" class=title > Vlasiator </a> </div> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item "><a href="/vlasiator_manual/" class="menu-list-link ">Home</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/" class="menu-list-link ">Installation</a> <ul class="menu-list-child-list "> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/#dccrg" class=menu-list-link >DCCRG</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/#zoltan" class=menu-list-link >Zoltan</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/#boost" class=menu-list-link >Boost</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/#eigen" class=menu-list-link >Eigen</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/#jemalloc" class=menu-list-link >Jemalloc</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/#phiprof" class=menu-list-link >PhiProf</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu1/#papi" class=menu-list-link >Papi</a> </ul> <li class="menu-list-item "><a href="/vlasiator_manual/menu2/" class="menu-list-link ">Run</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu3/" class="menu-list-link ">Postprocessing</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu4/" class="menu-list-link ">Internal</a> <ul class="menu-list-child-list "> <li class="menu-list-item "><a href="/vlasiator_manual/menu4/#sysboundary" class=menu-list-link >SysBoundary</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu4/#vlasov_solver" class=menu-list-link >Vlasov Solver</a> </ul> <li class="menu-list-item "><a href="/vlasiator_manual/menu5/" class="menu-list-link ">Test</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu6/" class="menu-list-link ">Misc</a> <li class="menu-list-item "><a href="/vlasiator_manual/menu7/" class="menu-list-link ">Method</a> <li class="menu-list-item active"><a href="/vlasiator_manual/menu8/" class="menu-list-link active">Trouble Shooting</a> </ul> </div> <div class=footer > This is <em>Just the docs</em>, adapted from the <a href="https://github.com/pmarsceill/just-the-docs" target=_blank >Jekyll theme</a>. </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <a id=github  href="/vlasiator_manual//github.com/fmihpc/vlasiator">Vlasiator on GitHub</a> </div> <div class=franklin-content ><h1 id=trouble_shooting ><a href="#trouble_shooting" class=header-anchor >Trouble Shooting</a></h1> <div class=franklin-toc ><ol><li><a href="#memory">Memory</a><li><a href="#timestepping">Timestepping</a><li><a href="#staggered_grid">Staggered Grid</a><li><a href="#velocity_space_resolution">Velocity Space Resolution</a><li><a href="#velocity_space_limit">Velocity Space Limit</a><li><a href="#ordinary_space_resolution">Ordinary Space Resolution</a><li><a href="#error_propagation">Error Propagation</a><li><a href="#amr_filtering">AMR Filtering</a><li><a href="#units">Units</a><li><a href="#magnetic_flux_conservation">Magnetic Flux Conservation</a><li><a href="#vlsv_writing_bug">VLSV Writing Bug</a><li><a href="#performance">Performance</a></ol></div> <h2 id=memory ><a href="#memory" class=header-anchor >Memory</a></h2> <p>Memory is often the bottleneck for Vlasiator. Vlasiator is not constant in memory usage: as the plasma system evolves from initial state, which is mostly described by a Maxwellian distribution, the system shifts away from thermal equilibrium states and the blocks it requires to resolve the nonMaxwellian distribution may drastically increase. The total block counts can easily increase 5x as the system evolves into a quasi-steady state.</p> <p>All kinds of weird error messages may pop up due to runtime out-of-memory error:</p> <ul> <li><p><code>Segmentation fault</code></p> <li><p><code>Out-of-memory</code></p> <li><p><code>malloc failed</code></p> <li><p><code>Bus error: nonexistent physical address</code></p> </ul> <p>Even if you set the maximum memory limit for bailing-out when compiled with PAPI in configuration files, it may still crash because of memory fragmentations. Check <code>memoryallocation.cpp</code> in the source code if you want to see the details.</p> <p>One thing to be careful about is that the initial memory usage reported by PAPI significantly underestimates the actual usage, e.g. by 25&#37;. It may actually be better to look at system memory usage for a better estimation.</p> <p>There are currently two ways to solve the out-of-memory issue:</p> <ol> <li><p>Replace some MPI processes with OpenMP threads.</p> <li><p>Run with larger memory.</p> </ol> <p>Multithreading runs can save memory mostly because there will be fewer ghost cells required for communication.</p> <h2 id=timestepping ><a href="#timestepping" class=header-anchor >Timestepping</a></h2> <p>In regions of small densities, the maximum characteristic wave speed may become very large, which in turn limits the discrete time steps constrained by the CFL condition. Surprisingly this has not become a major issue in Vlasiator 5.1, but I did suffer for a long time because of the problematic magnetic field settings near the inner boundary &#40;<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >∇</mi><mo>⋅</mo><mi mathvariant=bold >B</mi><mo mathvariant=normal >≠</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla\cdot\mathbf{B}\neq 0</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class=mord >∇</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >⋅</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class=mord ><span class="mord mathbf">B</span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel ><span class=mrel ><span class="mord vbox"><span class=thinbox ><span class=rlap ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class=inner ><span class=mrel ></span></span><span class=fix ></span></span></span></span></span><span class=mrel >=</span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >0</span></span></span></span>&#41;.</p> <p>Check some ideas in <a href="https://henry2004y.github.io/vlasiator_manual/menu7/#low_density_treatment">Low Density Treatment</a> if you are interested in implementing a practical solution.</p> <h2 id=staggered_grid ><a href="#staggered_grid" class=header-anchor >Staggered Grid</a></h2> <p>All DCCRG variables are stored at cell centers, while the electric and magnetic field are stored at edge and face centers, respectively. While performing some sensitive analysis such as parallel electric field, mixing <code>fs_e</code> with <code>vg_b_vol</code> may cause errors up to 1 order of magnitude. The important thing is <strong>not</strong> to process data saved at different spatial locations&#33;</p> <h2 id=velocity_space_resolution ><a href="#velocity_space_resolution" class=header-anchor >Velocity Space Resolution</a></h2> <p>Extra care is needed when resolving the velocity space. Velocity is the 1st moment integral of the distribution function, and temperature determines the width of the distribution: if your velocity is too small compared to the thermal width, you can only resolve part of the phase space; if the velocity space cells are too large, then the integral will become inaccurate.</p> <p>Even in 1D simulations, Vlasiator treats the other 2 spatial dimensions as 1 cell with periodic boundaries. If the velocity space grid is too small in these two extra dimensions, the sampling scheme used in Vlasiator 5.1 still generate significant discrepancy for the integrals. Under my tests, within 1&#37; error we need 6 blocks &#40;24 cells&#41; in the extra dimensions, and 10 blocks &#40;40 cells&#41; would be better.</p> <h2 id=velocity_space_limit ><a href="#velocity_space_limit" class=header-anchor >Velocity Space Limit</a></h2> <p>Currently Vlasiator will generate wrong physics when the velocity space distribution goes beyond the limit. A better question to ask will be: what is a proper boundary condition for the velocity space?</p> <h2 id=ordinary_space_resolution ><a href="#ordinary_space_resolution" class=header-anchor >Ordinary Space Resolution</a></h2> <p>A person coming from the PIC world may think that it requires resolving the ion inertial length to produce correct ion physics. However, one key claim from the Vlasiator experiments is that this is not necessarily true for a vlasov ion model: <em>we don&#39;t need to fully resolve the ion scales to produce ion physics</em>. For example, in Maxime&#39;s paper about mirror modes and EMIC waves in the meriodional plane 2D3V simulation, he showed that at a resolution of the ion scale 300km given the upstream parameters we could already capture ion-related wave pretty well.</p> <h2 id=error_propagation ><a href="#error_propagation" class=header-anchor >Error Propagation</a></h2> <p>I am not a fan of the error propagations used in Vlasiator: for most of the functions I see, they return <code>false</code> after something goes wrong, which are unecessary simply because the code will generate wrong results anyway. Error propagation is useful when we want to continue the service even if some parts go down, not in a situation like a physical model where any tiny error will generate completely wrong results.</p> <h2 id=amr_filtering ><a href="#amr_filtering" class=header-anchor >AMR Filtering</a></h2> <p>The Vlasov solver provides moments of the distribution function as input for the field solver, which are spatially filtered to minimize refinement artefacts. In Vlasiator 5.1 it was found to be buggy, but I know little about this currently.</p> <h2 id=units ><a href="#units" class=header-anchor >Units</a></h2> <p>It seems like SI units are used for both I/O and internal computations. Since electron is treated as massless fluid, no scaling of mass is present.</p> <p>Double precision is sufficient even if all the physical constants across our known scales are involved. However, if we use single precision floating numbers, we need to be careful about physical constants&#33;</p> <h2 id=magnetic_flux_conservation ><a href="#magnetic_flux_conservation" class=header-anchor >Magnetic Flux Conservation</a></h2> <p>When I performed a closed magnetic flux calculation for a 2D meridional run, I found that the total closed dayside magnetic flux is not conserved. Several possibilities:</p> <ul> <li><p>The inner boundary, with a fixed line dipole and a perturbed magnetic field shielded by a perfect conductor, acts as a source to maintain the flux inside the closed field region.</p> <li><p>Simply the numerical solver does not consider this at all.</p> <li><p>I made a mistake in extracting the last-closed field line or performing the integral.</p> </ul> <h2 id=vlsv_writing_bug ><a href="#vlsv_writing_bug" class=header-anchor >VLSV Writing Bug</a></h2> <p>The VLSV library has a <a href="https://github.com/fmihpc/vlasiator/issues/698">file writing bug</a> with OpenMPI4. This happens even for serial runs as long as the code is compiled with MPI. Current workaround it to set</p> <pre><code class="bash hljs"><span class=hljs-built_in >export</span> OMPI_MCA_io=^ompio</code></pre>
<h2 id=performance ><a href="#performance" class=header-anchor >Performance</a></h2>
<p>Vlasiator is distributed with MPI&#43;OpenMP, but the optimal MPI tasks &#43; OpenMP threads combination differs case by case. Hyperthreading may or may not help.</p>
<p>On many clusters with Slurm job scheduler, hyperthreading is on by adding</p>
<pre><code class="shell hljs"><span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --hint=multithread</span></code></pre>
<p>A general MPI &#43; OpenMP with simultaneous multithreading example Slurm script:</p>
<pre><code class="shell hljs"><span class="hljs-meta prompt_">#</span><span class=language-bash >!/bin/bash</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --job-name=example</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --account=&lt;project&gt;</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --partition=large</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --time=02:00:00</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --nodes=100</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --hint=multithread</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --ntasks-per-node=16</span>
<span class="hljs-meta prompt_">#</span><span class=language-bash >SBATCH --cpus-per-task=16</span>
<span class="hljs-meta prompt_">
# </span><span class=language-bash >Note that the ntasks-per-node * cpus-per-task = total core counts with hyperthreading</span>
<span class="hljs-meta prompt_">
# </span><span class=language-bash >Set the number of threads based on --cpus-per-task</span>
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

srun myprog &lt;options&gt;</code></pre>
<p>Slurm now comes with a efficiency report script called <code>seff</code>. An example output of <code>seff &#36;SLURM_JOBID</code> looks like</p>
<pre><code class="julia hljs">Job ID: <span class=hljs-number >766237</span>
Cluster: mahti
User/Group: hongyang/hongyang
State: RUNNING
Nodes: <span class=hljs-number >30</span>
Cores per node: <span class=hljs-number >256</span>
CPU Utilized: <span class=hljs-number >680</span>-<span class=hljs-number >03</span>:<span class=hljs-number >29</span>:<span class=hljs-number >31</span>
CPU Efficiency: <span class=hljs-number >14.69</span>% of <span class=hljs-number >4630</span>-<span class=hljs-number >20</span>:<span class=hljs-number >16</span>:<span class=hljs-number >00</span> core-walltime
Job Wall-clock time: <span class=hljs-number >14</span>:<span class=hljs-number >28</span>:<span class=hljs-number >17</span>
Memory Utilized: <span class=hljs-number >7.40</span> TB (estimated maximum)
Memory Efficiency: <span class=hljs-number >107.77</span>% of <span class=hljs-number >6.87</span> TB (<span class=hljs-number >234.38</span> GB/node)
Job consumed <span class=hljs-number >43414.17</span> CSC billing units based on following used resources
Billed project: project_2004873
Non-Interactive BUs: <span class=hljs-number >43414.17</span></code></pre>
<p>In general, if the processor usage is far below 100&#37;, the code may not be working efficiently; if the memory usage is far below 100&#37; or above 100&#37;, you may have a problem with the RAM requirements. In this particular case, we have very low CPU efficiency because we were not running with all the cores on a compute node and Vlasiator is a memory-bound program; we have over 100&#37; memory efficiency which indicates that the memory pressure is very high and we may need to consider increase the memory requirement.</p>
<div class=page-foot >
  <div class=copyright >
    &copy; Hongyang Zhou. Last modified: June 16, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div> 
    </div> 
    </div> <!-- end of class page-wrap-->